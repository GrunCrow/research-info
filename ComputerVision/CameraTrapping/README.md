# Camera Trapping

Literature related to the detection and classification of species from images captured by camera traps in the wilderness.

| Title | Authors | Year | Abstract | Review |
|-------|---------|------|----------|--------|
| [A first step towards automated species recognition from camera trap images of mammals using AI in a European temperate forest](https://link.springer.com/chapter/10.1007/978-3-030-84340-3_24) | Mateusz Choiński, Mateusz Rogowski, Piotr Tynecki, Dries P. J. Kuijper, Marcin Churski & Jakub W. Bubnicki | 2021 | Camera traps are used worldwide to monitor wildlife. Despite the increasing availability of Deep Learning (DL) models, the effective usage of this technology to support wildlife monitoring is limited. This is mainly due to the complexity of DL technology and high computing requirements. This paper presents the implementation of the light-weight and state-of-the-art YOLOv5 architecture for automated labeling of camera trap images of mammals in the Białowieża Forest (BF), Poland. The camera trapping data were organized and harmonized using TRAPPER software, an open-source application for managing large-scale wildlife monitoring projects. The proposed image recognition pipeline achieved an average accuracy of 85% F1-score in the identification of the 12 most commonly occurring medium-size and large mammal species in BF, using a limited set of training and testing data (a total of 2659 images with animals). Based on the preliminary results, we have concluded that the YOLOv5 object detection and classification model is a fine and promising DL solution after the adoption of the transfer learning technique. It can be efficiently plugged in via an API into existing web-based camera trapping data processing platforms such as e.g. TRAPPER system. Since TRAPPER is already used to manage and classify (manually) camera trapping datasets by many research groups in Europe, the implementation of AI-based automated species classification will significantly speed up the data processing workflow and thus better support data-driven wildlife monitoring and conservation. Moreover, YOLOv5 has been proven to perform well on edge devices, which may open a new chapter in animal population monitoring in real-time directly from camera trap devices. | Pipeline that includes YOLOv5 |
| [Automated detection of European wild mammal species in camera trap images with an existing and pre-trained computer vision model](https://link.springer.com/article/10.1007/s10344-020-01404-y) | Christin Carl, Fiona Schönfeld, Ingolf Profft, Alisa Klamm & Dirk Landgraf  | 2020 | The use of camera traps is a nonintrusive monitoring method to obtain valuable information about the appearance and behavior of wild animals. However, each study generates thousands of pictures and extracting information remains mostly an expensive, time-consuming manual task. Nevertheless, image recognition and analyzing technologies combined with machine learning algorithms, particularly deep learning models, improve and speed up the analysis process. Therefore, we tested the usability of a pre-trained deep learning model available on the TensorFlow hub–FasterRCNN+InceptionResNet V2 network applied to images of ten different European wild mammal species such as wild boar (Sus scrofa), roe deer (Capreolus capreolus), or red fox (Vulpes vulpes) in color as well as black and white infrared images. We found that the detection rate of the correct region of interest (region of the animal) was 94%. The classification accuracy was 71% for the correct species’ name as mammals and 93% for the correct species or higher taxonomic ranks such as “carnivore” as order. In 7% of cases, the classification was incorrect as the wrong species’ name was classified. In this technical note, we have shown the potential of an existing and pre-trained image classification model for wildlife animal detection, classification, and analysis. A specific training of the model on European wild mammal species could further increase the detection and classification accuracy of the models. Analysis of camera trap images could thus become considerably faster, less expensive, and more efficient. | Pre-trained deep learning model available on TensorFlow |
| [Automatic Camera-Trap Classification Using Wildlife-Specific Deep Learning in Nilgai Management](https://meridian.allenpress.com/jfwm/article/12/2/412/468297/Automatic-Camera-Trap-Classification-Using) | Matthew Kutugata; Jeremy Baumgardt; John A. Goolsby; Alexis E. Racelis | 2021 | Camera traps provide a low-cost approach to collect data and monitor wildlife across large scales but hand-labeling images at a rate that outpaces accumulation is difficult. Deep learning, a subdiscipline of machine learning and computer science, can address the issue of automatically classifying camera-trap images with a high degree of accuracy. This technique, however, may be less accessible to ecologists or small-scale conservation projects, and has serious limitations. In this study, we trained a simple deep learning model using a dataset of 120,000 images to identify the presence of nilgai Boselaphus tragocamelus, a regionally specific nonnative game animal, in camera-trap images with an overall accuracy of 97%. We trained a second model to identify 20 groups of animals and one group of images without any animals present, labeled as “none,” with an accuracy of 89%. Lastly, we tested the multigroup model on images collected of similar species, but in the southwestern United States, resulting in significantly lower precision and recall for each group. This study highlights the potential of deep learning for automating camera-trap image processing workflows, provides a brief overview of image-based deep learning, and discusses the often-understated limitations and methodological considerations in the context of wildlife conservation and species monitoring. |  |
| [A systematic study of the class imbalance problem: Automatically identifying empty camera trap images using convolutional neural networks](https://doi.org/10.1016/j.ecoinf.2021.101350) | Deng-Qi Yang, Tao Li, Meng-Tao Liu, Xiao-Wei Li, Ben-Hui Chen | 2021 | Camera traps, which are widely used in wildlife surveys, often produce massive images, and many of them are empty images not contain animals. Using the deep learning model to automatically identify the empty camera trap images can reduce the workload of manual classification significantly. However, the performance of deep learning models is easily affected by the class imbalance problem of training datasets, which is a common problem for actual wildlife survey projects. Almost all previous studies on empty image recognition used down-sampling or oversampling methods to eliminate the effect of class imbalance on the performance of deep learning classifiers. The class imbalance problem has been systematically studied in the field of traditional image recognition, yet very limited research is available in the context of identifying camera trap images taken from highly cluttered natural scenes. This study systematically studied the impact of class imbalance on model performance when using a deep learning model to identify empty camera trap images. Then we proposed the construction method of training sets of the deep learning model when the data set has different class imbalance levels. Based on results from our experiments we concluded that (i) the class imbalance showed little effect on the performance of the model when the empty image ratio (EIR) in the data set was between 10% and 70%, so the training sets can be randomly built without changing the class distribution; (ii) we recommended using oversampling to partially eliminate class imbalance to reduce omission errors when the EIR of the data set exceeded 70%; (iii) when the EIRs of the training set and the test set were close, the overall error, omission error, and commission error of the model were relatively smaller, and the model tended to achieve a better overall performance; (iv) the omission and commission errors can be adjusted by changing the percentage of empty images in the training set. |  |
<!--| Paper | Author | 2022| [Link 1](URL_del_paper_1) | Abstract | Comment |-->